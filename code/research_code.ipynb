{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Visualizing Research Trends with bar and line graphs",
   "id": "6ffa020aa4775778"
  },
  {
   "cell_type": "code",
   "id": "a4d7fe14400914ad",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-19T12:35:01.674018Z",
     "start_time": "2024-11-19T12:35:01.419612Z"
    }
   },
   "source": [
    "from tqdm import tqdm  # Import tqdm for progress bar\n",
    "import json\n",
    "import pandas as pd\n",
    "import ast\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ],
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tqdm'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtqdm\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m tqdm  \u001B[38;5;66;03m# Import tqdm for progress bar\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mjson\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mpd\u001B[39;00m\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'tqdm'"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def preprocess(filename):\n",
    "    \"\"\"\n",
    "    Preprocess on each txt file individually and after that we combine them\n",
    "    \"\"\"\n",
    "    file_path = filename\n",
    "\n",
    "    total_lines = sum(1 for _ in open(file_path, 'r', encoding='utf-8'))\n",
    "    data = []\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in tqdm(file, total=total_lines, desc=\"Processing file\"):\n",
    "            record = json.loads(line.strip())\n",
    "            data.append(record)\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    df = df.drop_duplicates(subset='id')\n",
    "    df.drop(columns=['venue', 'page_start', 'page_end', 'doc_type', 'publisher', 'issue', 'volume',\n",
    "                     'url', 'doi', 'indexed_abstract', 'references', 'abstract'], inplace=True, errors='ignore')\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    df = df[df['year'] >= 2000]\n",
    "    df = df[df['year'] < 2020]\n",
    "\n",
    "    def all_authors_have_name_and_id(author_list):\n",
    "        return all('org' in author and 'org_id' in author for author in author_list)\n",
    "\n",
    "    df = df[df['authors'].apply(all_authors_have_name_and_id)]\n",
    "\n",
    "    print(df.columns)\n",
    "\n",
    "    df.to_csv('mag_papers_preprocessed.csv', index=False, encoding='utf-8')\n",
    "    df.to_pickle('mag_papers_preprocessed.pkl')\n",
    "\n",
    "\n",
    "def combine_dataframes(folder_path):\n",
    "    file_list = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "    df_list = []\n",
    "\n",
    "    for file_name in file_list:\n",
    "        print(file_name)\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        df = pd.read_csv(file_path)\n",
    "        df = df.dropna(subset=['authors', 'fos'])\n",
    "        df['authors'] = df['authors'].apply(ast.literal_eval)\n",
    "        df['fos'] = df['fos'].apply(ast.literal_eval)\n",
    "        df_list.append(df)\n",
    "\n",
    "    # Concatenate all DataFrames into one DataFrame\n",
    "    combined_df = pd.concat(df_list, ignore_index=True)\n",
    "    combined_df.to_csv(f'{folder_path}/mag_papers_combined.csv')\n",
    "\n",
    "\n",
    "def save_to_pickle(df, filename='mag_papers_combined.pkl'):\n",
    "    \"\"\"Save DataFrame to a pickle file.\"\"\"\n",
    "    df.to_pickle(filename)\n",
    "\n",
    "\n",
    "def load_or_read_csv_or_pickle():\n",
    "    \"\"\"Load DataFrame from Pickle if available, otherwise read from CSV.\"\"\"\n",
    "    pickle_file = 'mag_papers_combined.pkl'  # this is the file with all the data\n",
    "\n",
    "    if os.path.exists(pickle_file):\n",
    "        df = pd.read_pickle(pickle_file)\n",
    "    else:\n",
    "        df = pd.read_csv('mag_papers_combined.csv', encoding='utf-8', low_memory=False)\n",
    "        save_to_pickle(df, pickle_file)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "######\n",
    "# bar plot\n",
    "######\n",
    "def process_fos_in_chunks_given_df(df, chunk_size=10000):\n",
    "    \"\"\"\n",
    "    Process the 'fos' column in chunks and return a Counter object.\n",
    "    :param df: the dataframe\n",
    "    :param chunk_size: how many records at a time\n",
    "    :return: Counter object\n",
    "    \"\"\"\n",
    "    fos_counter = Counter()\n",
    "\n",
    "    # divide the df into smaller chunks\n",
    "    for start in range(0, len(df), chunk_size):\n",
    "        end = min(start + chunk_size, len(df))\n",
    "        chunk = df.iloc[start:end]\n",
    "\n",
    "        chunk = chunk[pd.notna(chunk['fos'])]\n",
    "        chunk = chunk[chunk['year'] < 2020]\n",
    "        chunk['fos'] = chunk['fos'].apply(ast.literal_eval)\n",
    "\n",
    "        fos_names = [fos['name'] for fos_list in chunk['fos'] for fos in fos_list]\n",
    "        fos_counter.update(fos_names)\n",
    "\n",
    "    return fos_counter\n",
    "\n",
    "\n",
    "def viz_bar_graph(top_n=10, chunk_size=10000):\n",
    "    \"\"\"\n",
    "    visualize the bar graph\n",
    "    :param top_n: how many fields of study to visualize\n",
    "    :param chunk_size: how many records to process at a time\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    df = load_or_read_csv_or_pickle()\n",
    "\n",
    "    fos_counter = process_fos_in_chunks_given_df(df, chunk_size)\n",
    "\n",
    "    fos_df = pd.DataFrame(fos_counter.items(), columns=['fos_name', 'count'])\n",
    "    fos_df = fos_df.sort_values(by='count', ascending=False)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(fos_df['fos_name'][:top_n], fos_df['count'][:top_n], color='skyblue')\n",
    "    plt.xlabel('Fields of Study')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title(f'Top {top_n} Fields of Study Across All Years')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('viz.png')\n",
    "    plt.show()\n",
    "\n",
    "    # saving counter to json file for future use\n",
    "    with open('fos_counter.json', 'w') as f:\n",
    "        json.dump(fos_counter, f)\n",
    "\n",
    "\n",
    "def process_fos_in_chunks(chunk_size=10000):\n",
    "    \"\"\"\n",
    "    Process df in chunks and calculate FoS counts per year.\n",
    "    \"\"\"\n",
    "    fos_per_year = {}\n",
    "\n",
    "    df = pd.read_pickle('mag_papers_combined.pkl')\n",
    "\n",
    "    for i in range(0, len(df), chunk_size):\n",
    "        chunk = df.iloc[i:i + chunk_size]\n",
    "\n",
    "        chunk = chunk[pd.notna(chunk['fos'])]\n",
    "        chunk = chunk[chunk['year'] < 2020]\n",
    "        chunk['fos'] = chunk['fos'].apply(ast.literal_eval)\n",
    "\n",
    "        for year, group in chunk.groupby('year'):\n",
    "            fos_names = [fos['name'] for fos_list in group['fos'] for fos in fos_list]\n",
    "            if year not in fos_per_year:\n",
    "                fos_per_year[year] = Counter(fos_names)\n",
    "            else:\n",
    "                fos_per_year[year].update(fos_names)\n",
    "\n",
    "    return fos_per_year\n",
    "\n",
    "\n",
    "######\n",
    "# line plot\n",
    "######\n",
    "\n",
    "def process_fos_per_year_in_chunks(chunk_size=10000):\n",
    "    \"\"\"Process DataFrame in chunks and calculate FoS counts per year.\"\"\"\n",
    "    fos_per_year = {}\n",
    "\n",
    "    #df = pd.read_pickle('mag_papers_combined.pkl')\n",
    "    df = load_or_read_csv_or_pickle()\n",
    "\n",
    "    # Process the df in chunks\n",
    "    for i in range(0, len(df), chunk_size):\n",
    "        chunk = df.iloc[i:i + chunk_size]\n",
    "\n",
    "        chunk = chunk[pd.notna(chunk['fos'])]\n",
    "        chunk = chunk[chunk['year'] < 2020]\n",
    "        chunk['fos'] = chunk['fos'].apply(ast.literal_eval)\n",
    "\n",
    "        for year, group in chunk.groupby('year'):\n",
    "            fos_names = [fos['name'] for fos_list in group['fos'] for fos in fos_list]\n",
    "            if year not in fos_per_year:\n",
    "                fos_per_year[year] = Counter(fos_names)\n",
    "            else:\n",
    "                fos_per_year[year].update(fos_names)\n",
    "\n",
    "    return fos_per_year\n",
    "\n",
    "\n",
    "def plot_fos_trends(fos_per_year, top_n=10):\n",
    "    \"\"\"\n",
    "    Plot the trends for the top N Fields of Study over time.\n",
    "    :param fos_per_year: the dictionary with Fields of Study counts per year\n",
    "    :param top_n:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    total_fos_counter = Counter()\n",
    "    for year_counter in fos_per_year.values():\n",
    "        total_fos_counter.update(year_counter)\n",
    "\n",
    "    top_fos = [fos for fos, count in total_fos_counter.most_common(top_n)]\n",
    "\n",
    "    year_range = sorted(fos_per_year.keys())\n",
    "    year_range = year_range[:-1]\n",
    "    fos_trends = {fos: [] for fos in top_fos}\n",
    "\n",
    "    for year in year_range:\n",
    "        if int(year) < 2000 or int(year) > 2019:\n",
    "            continue\n",
    "        year_counter = fos_per_year[year]\n",
    "        for fos in top_fos:\n",
    "            fos_trends[fos].append(year_counter.get(fos, 0))\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for fos in top_fos:\n",
    "        plt.plot(year_range, fos_trends[fos], label=fos)\n",
    "\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title(f'Trends for Top {top_n} Fields of Study Over Time')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('fos_trends.png')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def save_fos_per_year_to_file(fos_per_year, filename='fos_per_year.json'):\n",
    "    \"\"\"Save the fos_per_year dictionary to a JSON file.\"\"\"\n",
    "    if os.path.exists(filename):\n",
    "        return\n",
    "    with open(filename, 'w') as f:\n",
    "        fos_per_year_serializable = {int(year): dict(counter) for year, counter in fos_per_year.items()}\n",
    "        json.dump(fos_per_year_serializable, f)\n",
    "\n",
    "\n",
    "def viz_with_trends(top_n=10, chunk_size=10000, save_file='fos_per_year.json'):\n",
    "    \"\"\"\n",
    "    Load file and Visualize the trends for the top N Fields of Study over time.\n",
    "    :param top_n: how many fields of study to visualize\n",
    "    :param chunk_size: how many records to process at a time\n",
    "    :param save_file: where to save the processed data\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    fos_per_year = process_fos_per_year_in_chunks(chunk_size)\n",
    "\n",
    "    # saving it to a file for future use\n",
    "    save_fos_per_year_to_file(fos_per_year, save_file)\n",
    "    plot_fos_trends(fos_per_year, top_n)\n"
   ],
   "id": "2ed4370ee318b74f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Run the following code to visualize the research trends with bar and line graphs.",
   "id": "a1016fb5c8c63d7a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "filename = 'mag_papers.txt'  # choose the file\n",
    "preprocess(filename)\n",
    "folder_name = ''\n",
    "combine_dataframes(folder_name)\n",
    "viz_bar_graph(top_n=10)\n",
    "viz_with_trends(top_n=10)"
   ],
   "id": "initial_id"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Community Detection in Research Fields",
   "id": "fd10488d2d7bc3c2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import re\n",
    "from networkx.algorithms.community import louvain_communities\n",
    "from collections import Counter"
   ],
   "id": "b766c332bd9bc0b2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_fos_names(fos_list):\n",
    "    \"\"\"\n",
    "    Extracts fields of science (FoS) names from a string containing a list-like\n",
    "    structure and returns a list of extracted FoS names as strings\n",
    "    \"\"\"\n",
    "    reg = r\"'[a-zA-Z ]+',\"\n",
    "    names = re.findall(reg, fos_list)\n",
    "    return [name[1:-2] for name in names]\n",
    "\n",
    "\n",
    "def get_fos_counts_per_community(communities, data):\n",
    "    \"\"\"\n",
    "    Count the frequency of FOS terms for each community\n",
    "\n",
    "    This function is used to determine the labels for the communities by printing \n",
    "    the top 10 most common FOS terms and their counts for each community. \n",
    "    Although it is not part of the community detection pipeline, it is left here for \n",
    "    documentation purposes to help in understanding and interpreting community labels.\n",
    "    \"\"\"\n",
    "    community_fos_counts = []\n",
    "\n",
    "    for community_idx, community in enumerate(communities):\n",
    "        fos_in_community = []\n",
    "        for index, row in data.iterrows():\n",
    "            fields_of_study = row['fos_names'].split(', ')\n",
    "\n",
    "            # Add to the community if the FOS terms belong to the community\n",
    "            for fos in fields_of_study:\n",
    "                if fos in community:\n",
    "                    fos_in_community.append(fos)\n",
    "\n",
    "        fos_count = Counter(fos_in_community)  # Count the occurrences of each FOS in the community\n",
    "        community_fos_counts.append(fos_count)\n",
    "\n",
    "    return community_fos_counts\n",
    "\n",
    "\n",
    "def print_most_common_fos_in_communities(community_fos_counts):\n",
    "    \"\"\"\n",
    "    Print the most common FOS terms for each community.\n",
    "\n",
    "    This function is used to determine the labels for the communities by printing \n",
    "    the top 10 most common FOS terms and their counts for each community. \n",
    "    Although it is not part of the community detection pipeline, it is left here for \n",
    "    documentation purposes to help in understanding and interpreting community labels.\n",
    "    \"\"\"\n",
    "    for community_idx, fos_counter in enumerate(community_fos_counts):\n",
    "        common_fos = fos_counter.most_common(10)\n",
    "        common_fos_str = ', '.join([f'{fos} ({count})' for fos, count in common_fos])\n",
    "        print(f\"Community {community_idx}: {common_fos_str}\")\n",
    "\n",
    "\n",
    "def collaboration_graph(data):\n",
    "    \"\"\"\n",
    "    Constructs a collaboration graph where each node is a field of science (FoS),\n",
    "    and an edge between nodes represents co-occurrence of FoS in the same paper.\n",
    "    The weight of the edge corresponds to the number of co-occurrences.\n",
    "    \"\"\"\n",
    "    G = nx.Graph()\n",
    "\n",
    "    for index, row in data.iterrows():\n",
    "        fields_of_study = row['fos_names'].split(', ')\n",
    "\n",
    "        # Add edges between all fields in this paper\n",
    "        for i in range(len(fields_of_study)):\n",
    "            for j in range(i + 1, len(fields_of_study)):\n",
    "                G.add_edge(fields_of_study[i],\n",
    "                           fields_of_study[j],\n",
    "                           weight=G.get_edge_data(fields_of_study[i],\n",
    "                                                  fields_of_study[j],\n",
    "                                                  default={'weight': 0})['weight'] + 1)\n",
    "    return G\n",
    "\n",
    "\n",
    "def get_aggregated_graph(G, communities):\n",
    "    \"\"\"\n",
    "    Aggregates the original collaboration graph by collapsing nodes\n",
    "    into their respective communities\n",
    "    \"\"\"\n",
    "    aggregated_graph = nx.Graph()\n",
    "    community_map = {}\n",
    "\n",
    "    # aAsign nodes to communities\n",
    "    for i, community in enumerate(communities):\n",
    "        aggregated_graph.add_node(i, size=len(community))\n",
    "        for node in community:\n",
    "            community_map[node] = i\n",
    "\n",
    "    # Add edges between communities\n",
    "    for u, v in G.edges():\n",
    "        u_community = community_map[u]\n",
    "        v_community = community_map[v]\n",
    "        if u_community != v_community:\n",
    "            if aggregated_graph.has_edge(u_community, v_community):\n",
    "                aggregated_graph[u_community][v_community]['weight'] += 1\n",
    "            else:\n",
    "                aggregated_graph.add_edge(u_community, v_community, weight=1)\n",
    "\n",
    "    return aggregated_graph\n",
    "\n",
    "\n",
    "def plot_communities(G, communities, title, path, labels):\n",
    "    \"\"\"\n",
    "    Visualizes the aggregated graph of communities\n",
    "    \"\"\"\n",
    "    pos = nx.circular_layout(G)\n",
    "\n",
    "    colors = cm.rainbow(np.linspace(0, 1, len(communities)))\n",
    "    node_colors = [colors[i] for i in range(len(communities))]\n",
    "    sizes = [G.nodes[node]['size'] for node in G.nodes()]\n",
    "    node_color_map = [node_colors[node] for node in G.nodes()]\n",
    "    weights = [G[u][v]['weight'] * 0.0001 for u, v in G.edges()]\n",
    "\n",
    "    plt.figure(figsize=(15, 15))\n",
    "\n",
    "    nx.draw_networkx_nodes(G,\n",
    "                           pos,\n",
    "                           node_size=sizes,\n",
    "                           node_color=node_color_map)\n",
    "\n",
    "    nx.draw_networkx_edges(G,\n",
    "                           pos,\n",
    "                           width=weights)\n",
    "\n",
    "    nx.draw_networkx_labels(G, pos, labels, font_size=12)\n",
    "\n",
    "    plt.title(title, fontsize=20)\n",
    "\n",
    "    plt.savefig(path)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def get_community_labels(communities):\n",
    "    \"\"\"\n",
    "    Generates labels for each community based on the fields of s\n",
    "    cience present within the community\n",
    "    \"\"\"\n",
    "    labels = dict()\n",
    "    for community_idx, community in enumerate(communities):\n",
    "        fields = []\n",
    "        if 'mathematics' in community:\n",
    "            fields.append('Mathematics')\n",
    "        if 'physics' in community:\n",
    "            fields.append('Physics')\n",
    "        if 'chemistry' in community:\n",
    "            fields.append('Chemistry')\n",
    "        if 'materials science' in community:\n",
    "            fields.append('Materials Science')\n",
    "        if 'medicine' in community:\n",
    "            fields.append('Medicine')\n",
    "        if 'biology' in community:\n",
    "            fields.append('Biology')\n",
    "        if 'humanities' in community:\n",
    "            fields.append('Social Science\\nand Humanities')\n",
    "        if 'engineering' in community:\n",
    "            fields.append('Engineering')\n",
    "        if 'geology' in community:\n",
    "            fields.append('Geology')\n",
    "        if 'environmental science' in community:\n",
    "            fields.append('Environmental Science')\n",
    "        if 'computer science' in community:\n",
    "            fields.append('Computer Science')\n",
    "\n",
    "        if len(fields) == 0:\n",
    "            labels[community_idx] = ',\\n'.join(list(community)[:3])\n",
    "        else:\n",
    "            labels[community_idx] = '\\n'.join(fields)\n",
    "\n",
    "    return labels"
   ],
   "id": "7aa0185467a2b408"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Run the following code to detect communities in research fields and visualize them.",
   "id": "21cd8be0630ae3fe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "mag_papers = pd.read_csv('mag_papers_combined.csv')\n",
    "mag_papers = mag_papers.dropna(subset=['id', 'title', 'authors', 'year', 'fos'])\n",
    "mag_papers['fos_names'] = mag_papers['fos'].apply(get_fos_names)\n",
    "\n",
    "fos_data = mag_papers[['year', 'fos_names']]\n",
    "fos_data['fos_names'] = fos_data['fos_names'].apply(lambda x: ', '.join(x))\n",
    "\n",
    "for year in range(2000, 2020):\n",
    "    collaboration_G = collaboration_graph(fos_data[fos_data['year'] == year])\n",
    "    communities = louvain_communities(collaboration_G)\n",
    "    aggregated_graph = get_aggregated_graph(collaboration_G, communities)\n",
    "    labels = get_community_labels(communities)\n",
    "    plot_communities(aggregated_graph,\n",
    "                     communities,\n",
    "                     title=f'Louvain Communities for Year {year}',\n",
    "                     path=f'plots/louvain_{year}.png',\n",
    "                     labels=labels)"
   ],
   "id": "25803b82d4c73b00"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Clustering Research Papers with K-Means",
   "id": "a47b65a418abf29a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import ast\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from collections import defaultdict, Counter\n",
    "from networkx.algorithms.community import louvain_communities"
   ],
   "id": "f71cb7d267ea4cd8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def kmeans_clustering(data, X, n_clusters):\n",
    "    fos_list = data['fos_names']\n",
    "\n",
    "    # K-means clustering to group fields into general categories\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    data[f'category_cluster_{n_clusters}'] = kmeans.fit_predict(X)\n",
    "\n",
    "    clusters = kmeans.labels_  # cluster labels for each keyword\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=int(n_clusters / 2),\n",
    "                             ncols=2,\n",
    "                             figsize=(20, 40))\n",
    "\n",
    "    clusters_dict = dict()\n",
    "\n",
    "    for cluster, ax in enumerate(axes.flatten()):\n",
    "        terms_in_cluster = ', '.join(fos_list[data[f'category_cluster_{n_clusters}'] == cluster])\n",
    "        terms_dict = Counter(terms_in_cluster.split(', '))\n",
    "        common_terms = ', '.join([term for term, count in terms_dict.most_common(10)])\n",
    "        clusters_dict[cluster] = common_terms\n",
    "\n",
    "        # Plot a word cloud for this cluster\n",
    "        wordcloud = WordCloud(width=800,\n",
    "                              height=400,\n",
    "                              background_color='white').generate_from_frequencies(terms_dict)\n",
    "\n",
    "        ax.imshow(wordcloud, interpolation='bilinear')\n",
    "        ax.set_title(f'Cluster {cluster}', fontsize=12)\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'cloud_{n_clusters}_clusters.png')\n",
    "    plt.show()\n",
    "\n",
    "    return clusters_dict\n",
    "\n",
    "\n",
    "def custom_tokenizer(text):\n",
    "    return [token.strip() for token in text.split(', ')]\n",
    "\n",
    "\n",
    "def clustering(fos_data):\n",
    "    vectorizer = TfidfVectorizer(tokenizer=custom_tokenizer, token_pattern=None)\n",
    "    X = vectorizer.fit_transform(fos_data['fos_names'])\n",
    "\n",
    "    clusters_common_terms = dict()\n",
    "\n",
    "    for n_clusters in [18, 14, 12, 10]:\n",
    "        clusters = kmeans_clustering(fos_data, X, n_clusters=n_clusters)\n",
    "        clusters_common_terms[n_clusters] = clusters\n",
    "        print()\n",
    "\n",
    "\n",
    "def plot_word_clouds(start_year, end_year):\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20, 10))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, year in enumerate(range(start_year, end_year)):\n",
    "        df_year = mag_papers[mag_papers['year'] == year]\n",
    "        df_year['fos'] = df_year['fos'].apply(ast.literal_eval)\n",
    "\n",
    "        fos_weight_dict = dict()\n",
    "\n",
    "        for fos_list in df_year['fos']:\n",
    "            for fos_dict in fos_list:\n",
    "                name = fos_dict['name']\n",
    "                w = fos_dict['w']\n",
    "                if name in fos_weight_dict:\n",
    "                    fos_weight_dict[name] += w\n",
    "                else:\n",
    "                    fos_weight_dict[name] = w\n",
    "\n",
    "        wordcloud = WordCloud(width=800,\n",
    "                              height=400,\n",
    "                              background_color='white').generate_from_frequencies(fos_weight_dict)\n",
    "\n",
    "        axes[i].imshow(wordcloud, interpolation='bilinear')\n",
    "        axes[i].set_title(f'{year}', fontsize=16)\n",
    "        axes[i].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'word_clouds_{start_year}_{end_year}.png')\n",
    "    plt.show()"
   ],
   "id": "5c22a898698832fd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Run the following code to cluster research papers with K-Means and plot word clouds.",
   "id": "bbf3af1d55f221fb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "mag_papers = pd.read_csv('mag_papers_combined.csv')\n",
    "mag_papers = mag_papers.dropna(subset=['id', 'title', 'authors', 'year', 'fos'])\n",
    "mag_papers['fos_names'] = mag_papers['fos'].apply(get_fos_names)\n",
    "\n",
    "for year in range(2000, 2020, 2):\n",
    "    plot_word_clouds(year, year + 2)"
   ],
   "id": "e77a3922fc972e9f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
